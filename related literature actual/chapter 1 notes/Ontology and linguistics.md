automatic semantic annotation is really nice for NERS
RDF Triplestore
concept representation in a computing medium

How word embedding works and the processes behind them in order to train deep learning models
- Bag of words: Extracts features from the text
- TF-IDF: Information retrieval, keyword extraction
- Word2Vec: Semantic analysis task
- GloVe: Word analogy, named entity recognition tasks
- BERT: language translation, question answering system

[1]Mingfang Wu, Hans Brandhorst, Maria-Cristina Marinescu, Joaquim More Lopez, Margorie Hlava, and Joseph Busch. 2023. Automated metadata annotation: What is and is not possible with machine learning. Data Intelligence 5, 1 (2023), 122–138.

This source says that NER's can be used to annotate image metadata using text description of said image.

[2]Amy Siu and Gerhard Weikum. 2015. Semantic type classification of common words in biomedical noun phrases. In Proceedings of BioNLP 15, 98–103.

Uses common nouns and nouns phrases and tries to add context to this phrase given medical record. They used a weighting system to see which context it would approve.

[3]Maite Oronoz, Arantza Casillas, Koldo Gojenola, and Alicia Perez. 2013. Automatic annotation of medical records in Spanish with disease, drug and substance names. In Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications: 18th Iberoamerican Congress, CIARP 2013, Havana, Cuba, November 20-23, 2013, Proceedings, Part II 18, Springer, 536–543.

This presents the danger of using common nouns as it will explode in ambiguity. Notice how it's always medical records.

[4]Pinaki Sinha and Ramesh Jain. 2008. Classification and annotation of digital photos using optical context data. In Proceedings of the 2008 international conference on Content-based image and video retrieval, 309–318.

The reason he used photo metadata is because it barely has context around it. It still holds up to modern studies that's why they have to tone down the common nouns.

